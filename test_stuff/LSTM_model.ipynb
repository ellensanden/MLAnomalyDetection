{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd091765243d447bc10b34282cff786081469c78a527bea45126a87448256c6538f",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## LSTM AUTOENCODER"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "process() takes 1 positional argument but 2 were given",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ea84f4032bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'gear_dataset.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mn_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: process() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from data_processing import process\n",
    "filename = 'gear_dataset.csv'\n",
    "rows = 1000\n",
    "data = process(filename,rows)\n",
    "n_rows = data.shape[0]\n",
    "n_features = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from numpy import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overlapping_window import overlap_window\n",
    "\n",
    "time_steps = 40\n",
    "a = np.r_[0:n_rows]\n",
    "\n",
    "X_train_samples = overlapping_window(time_steps,20,a)\n",
    "X_train = data[X_train_samples,:]\n",
    "X_train = np.squeeze(X_train)\n",
    "# print(X_train.shape)\n",
    "\n",
    "X_reversed = np.flip(X_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"EncoderDecoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 40, 64)]     0                                            \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 40, 256)      16640       input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 40, 256)      0           dense_3[0][0]                    \n__________________________________________________________________________________________________\nlstm_4 (LSTM)                   (None, 40, 128)      197120      dropout_1[0][0]                  \n__________________________________________________________________________________________________\nlstm_5 (LSTM)                   [(None, 40, 128), (N 131584      lstm_4[0][0]                     \n__________________________________________________________________________________________________\nlstm_6 (LSTM)                   (None, 40, 128)      131584      lstm_5[0][0]                     \n                                                                 lstm_5[0][1]                     \n                                                                 lstm_5[0][2]                     \n__________________________________________________________________________________________________\nlstm_7 (LSTM)                   (None, 40, 128)      131584      lstm_6[0][0]                     \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 40, 256)      33024       lstm_7[0][0]                     \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 40, 64)       16448       dense_4[0][0]                    \n==================================================================================================\nTotal params: 657,984\nTrainable params: 657,984\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_samples = X_train.shape[0]\n",
    "time_steps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "\n",
    "lstm_initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)\n",
    "\n",
    "encoderLSTM = LSTM(128,return_sequences=True,kernel_initializer =lstm_initializer, recurrent_initializer=lstm_initializer)\n",
    "\n",
    "# define Encoder\n",
    "EncoderInputs = Input(shape=(time_steps,n_features))\n",
    "dense1 =Dense(256, activation='tanh')(EncoderInputs)\n",
    "dropout = Dropout(0.2)(dense1)\n",
    "lstm1 = encoderLSTM(dropout)\n",
    "lstm2, state_h, state_c = LSTM(128,return_sequences=True,return_state=True,kernel_initializer =lstm_initializer, recurrent_initializer=lstm_initializer)(lstm1)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# define Decoder\n",
    "lstm3 =  LSTM(128,return_sequences=True,kernel_initializer =lstm_initializer, recurrent_initializer=lstm_initializer)(lstm2,initial_state=encoder_states)\n",
    "lstm4 = LSTM(128,return_sequences=True,kernel_initializer =lstm_initializer, recurrent_initializer=lstm_initializer)(lstm3)\n",
    "dense2 = Dense(256, activation='sigmoid')(lstm4)\n",
    "output = Dense(n_features,activation= 'sigmoid')(dense2)\n",
    "\n",
    "EncoderDecoder = Model(inputs=EncoderInputs, outputs=output,name=\"EncoderDecoder\")\n",
    "EncoderDecoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "EncoderDecoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-04d7b4ffa987>, line 33)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-04d7b4ffa987>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    seq, state_h, state_c = LSTM(units=lstm_units,return_sequences=True,return_state=True,kernel_initializer =lstm_initializer,recurrent_initializer=lstm_initializer,(inputDecoder,initial_state=encoder_states) # tune initializers?\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def model_builder(hp):\n",
    "   # do the lstm layers need to have the same number of units? will do that\n",
    "   # do the two dense layers need to have the same number of units?\n",
    "    n_samples = X_train.shape[0]\n",
    "    time_steps = X_train.shape[1]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "\n",
    "    lstm_initializer = tf.keras.initializers.RandomUniform(minval=-0.5, maxval=0.5)\n",
    "    \n",
    "    lstm_units = hp.Int('units', min_value=1, max_value=500, step=10)\n",
    "\n",
    "    # define Encoder\n",
    "    EncoderInputs = Input(shape=(time_steps,n_features))\n",
    "    \n",
    "    dense_units = hp.Int('units', min_value=10, max_value=5000, step=50)\n",
    "    dense1 =Dense(units=dense_units, activation='tanh')(EncoderInputs)\n",
    "    \n",
    "    dropoutP = hp.Float('dropout',min_value=0.0, max_value=0.95, step=0.05)\n",
    "    dropout = Dropout(dropoutP)(dense1) \n",
    "\n",
    "    inputLSTM = dropout\n",
    "    # Note that all parameter names should be unique (here, in the loop over i, we name the inner parameters 'units_' + str(i))\n",
    "    for i in range(hp.Int('n_LSTM_layers', 1, 10)):\n",
    "        #x = layers.LSTM(units=hp.Int('units_' + str(i), min_value=1, max_value=500, step=10), activation='sigmoid')(inputLSTM)\n",
    "        seq, state_h, state_c = LSTM(units=lstm_units,return_sequences=True,return_state=True,kernel_initializer = lstm_initializer, recurrent_initializer=lstm_initializer)(inputLSTM)\n",
    "        inputLSTM = seq\n",
    "\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    #decoder\n",
    "    for i in range(hp.Int('n_LSTM_layers', 1, 10)):\n",
    "        seq, state_h, state_c = LSTM(units=lstm_units,return_sequences=True,return_state=True,kernel_initializer =lstm_initializer,recurrent_initializer=lstm_initializer,(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
    "        inputDecoder = seq\n",
    "        encoder_states = [state_h,state_c]\n",
    "    \n",
    "    dense2_units = hp.Int('units', min_value=10, max_value=5000, step=50)\n",
    "    dense2 = Dense(units=dense2_units, activation='sigmoid')(lstm4)\n",
    "    \n",
    "    output = Dense(n_features,activation= 'sigmoid')(dense2)\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    EncoderDecoder = Model(inputs=EncoderInputs, outputs=output,name=\"EncoderDecoder\")\n",
    "    EncoderDecoder.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryCrossentropy()])\n",
    "    #EncoderDecoder.summary()\n",
    "  \n",
    "\n",
    "    return EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\LSTM\\oracle.json\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ellsan\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-26-9dd9ba830787>\", line 43, in model_builder\n",
      "    recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
      "UnboundLocalError: local variable 'inputDecoder' referenced before assignment\n",
      "Invalid model 0/5\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ellsan\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-26-9dd9ba830787>\", line 43, in model_builder\n",
      "    recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
      "UnboundLocalError: local variable 'inputDecoder' referenced before assignment\n",
      "Invalid model 1/5\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ellsan\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-26-9dd9ba830787>\", line 43, in model_builder\n",
      "    recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
      "UnboundLocalError: local variable 'inputDecoder' referenced before assignment\n",
      "Invalid model 2/5\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ellsan\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-26-9dd9ba830787>\", line 43, in model_builder\n",
      "    recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
      "UnboundLocalError: local variable 'inputDecoder' referenced before assignment\n",
      "Invalid model 3/5\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ellsan\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-26-9dd9ba830787>\", line 43, in model_builder\n",
      "    recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
      "UnboundLocalError: local variable 'inputDecoder' referenced before assignment\n",
      "Invalid model 4/5\n",
      "Invalid model 5/5\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ellsan\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\", line 104, in build\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"<ipython-input-26-9dd9ba830787>\", line 43, in model_builder\n",
      "    recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n",
      "UnboundLocalError: local variable 'inputDecoder' referenced before assignment\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mmaybe_distribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-9dd9ba830787>\u001b[0m in \u001b[0;36mmodel_builder\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m     42\u001b[0m                                     \u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mlstm_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                                     recurrent_initializer=lstm_initializer)(inputDecoder,initial_state=encoder_states) # tune initializers?\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0minputDecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'inputDecoder' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f60ff5a38388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m tuner = kt.Hyperband(model_builder,\n\u001b[0m\u001b[0;32m      3\u001b[0m                      \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                      \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\tuners\\hyperband.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mtune_new_entries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtune_new_entries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             allow_new_entries=allow_new_entries)\n\u001b[1;32m--> 344\u001b[1;33m         super(Hyperband, self).__init__(\n\u001b[0m\u001b[0;32m    345\u001b[0m             \u001b[0moracle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mhypermodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m                  \u001b[0mexecutions_per_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                  **kwargs):\n\u001b[1;32m---> 58\u001b[1;33m         super(MultiExecutionTuner, self).__init__(\n\u001b[0m\u001b[0;32m     59\u001b[0m             oracle, hypermodel, **kwargs)\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite)\u001b[0m\n\u001b[0;32m     98\u001b[0m                 distribution_strategy=distribution_strategy)\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         super(Tuner, self).__init__(oracle=oracle,\n\u001b[0m\u001b[0;32m    101\u001b[0m                                     \u001b[0mhypermodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                                     \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, logger, overwrite)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_populate_initial_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moverwrite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tuner_fname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36m_populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m    104\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36m_build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;31m# to the search space.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\kerastuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_fail_streak\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                     raise RuntimeError(\n\u001b[0m\u001b[0;32m    113\u001b[0m                         'Too many failed attempts to build model.')\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Too many failed attempts to build model."
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     project_name='LSTM')\n",
    "\n",
    "es= EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "tuner.search(X_train, X_train, epochs=50, validation_split=0.2, callbacks=[es])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'EncoderDecoder' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d007deafda9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EncoderDecoder' is not defined"
     ]
    }
   ],
   "source": [
    "n_samples = X_train.shape[0]\n",
    "time_steps = X_train.shape[1]\n",
    "n_features = X_train.shape[2]\n",
    "import time\n",
    "train_size = int(np.floor(0.7*n_samples))\n",
    "\n",
    "model = EncoderDecoder\n",
    "es= EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
    "\n",
    "s=time.time()\n",
    "\n",
    "history = model.fit(X_train[0:train_size,:,:], X_train[0:train_size,:,:], validation_data=(X_train[train_size:,:,:], X_train[train_size:,:,:]), epochs=300, verbose=2, shuffle=False, callbacks = [es])\n",
    "\n",
    "e=time.time()\n",
    "\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "print(f'training time = {e-s} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}