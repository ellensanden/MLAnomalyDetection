{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Import and format data\n",
    "## These  (until *) don't need to be run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# names = ['Time','ID','Data']\n",
    "read_file = pd.read_csv(\"candump-2021-02-08_150302.log\", header = None)\n",
    "read_file.to_csv (r'can_data.csv', index=None)\n",
    "can_data = pd.read_csv(\"can_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_data"
   ]
  },
  {
   "source": [
    "### remove vcan0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"can_data.csv\", \"r\")\n",
    "text = ''.join([i for i in text]) \\\n",
    "    .replace(\"vcan0\", \",\")\n",
    "x = open(\"can_data.csv\",\"w\")\n",
    "x.writelines(text)\n",
    "x.close()"
   ]
  },
  {
   "source": [
    "### remove hash"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"can_data.csv\", \"r\")\n",
    "text = ''.join([i for i in text]) \\\n",
    "    .replace(\"#\", \",\")\n",
    "x = open(\"can_data.csv\",\"w\")\n",
    "x.writelines(text)\n",
    "x.close()"
   ]
  },
  {
   "source": [
    "### remove parenthesis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"can_data.csv\", \"r\")\n",
    "text = ''.join([i for i in text]) \\\n",
    "    .replace(\"(\", \"\")\n",
    "x = open(\"can_data.csv\",\"w\")\n",
    "x.writelines(text)\n",
    "x.close()\n",
    "\n",
    "text = open(\"can_data.csv\", \"r\")\n",
    "text = ''.join([i for i in text]) \\\n",
    "    .replace(\")\", \"\")\n",
    "x = open(\"can_data.csv\",\"w\")\n",
    "x.writelines(text)\n",
    "x.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_data = pd.read_csv(\"can_data.csv\",names = ['Time','ID','Data'])\n",
    "can_data.to_csv (r'can_data.csv', index=None)\n",
    "print(can_data)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# *"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "can_data = pd.read_csv(\"can_data.csv\")\n",
    "print(can_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time = can_data['Time']\n",
    "Time = Time[1:-1]\n",
    "Time = Time.reset_index(drop=True)\n",
    "\n",
    "ID = can_data['ID']\n",
    "ID = ID[1:-1]\n",
    "ID = ID.reset_index(drop=True)\n",
    "\n",
    "Data = can_data['Data']\n",
    "Data = Data[1:-1]\n",
    "Data = Data.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def delta_time(Time): # calculates the time between two subsequent messages\n",
    "    delta = np.zeros((Time.size))\n",
    "    for x in range(Time.size-1):\n",
    "\n",
    "       delta[x] = Time[x+1]-Time[x]\n",
    "\n",
    "    return delta   \n",
    "\n",
    "delta = delta_time(Time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def convert_from_hex(hex,output_type): # converts the data in hex from hexadecimal to decimal form\n",
    "     out = np.zeros((hex.size))\n",
    "     if output_type == 'dec':\n",
    "        for x in range(hex.size):\n",
    "            h_value = hex[x]\n",
    "            out[x] = int(h_value,16)\n",
    "     else:\n",
    "        for x in range(hex.size):\n",
    "            h_value = hex[x]\n",
    "            binary[x] = bin(int(h_value, 16))[2:]\n",
    "\n",
    "     return out\n",
    "\n",
    "\n",
    "data = convert_from_hex(Data,'dec')\n",
    "id = convert_from_hex(ID,'dec')"
   ]
  },
  {
   "source": [
    "## Normalize data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = data.reshape(-1, 1)\n",
    "#data = scaler.fit_transform(data)\n",
    "id = id.reshape(-1, 1)\n",
    "id = scaler.fit_transform(id)"
   ]
  },
  {
   "source": [
    "## Visualize data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Time[0:10000],data[0:10000],'o')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Data')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Time[0:10000],id[0:10000],'o')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ID')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data[0:10000],id[0:10000],'o')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('ID')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(Time.size)\n",
    "plt.figure()\n",
    "plt.plot(steps[0:100000],delta[0:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_IDs(id): # returns groups of indices of unique ids in list \n",
    "\n",
    "    idx_sort = np.argsort(id)\n",
    "    sorted_ids = id[idx_sort]\n",
    "    vals, idx_start, count = np.unique(sorted_ids, return_counts=True, return_index=True)\n",
    "    indices = np.split(idx_sort, idx_start[1:])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_id = np.transpose(id) # not sure why it needs to be transposed but it doesn't work otherwise\n",
    "indices = sort_IDs(transposed_id[0][:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_stream(indices,data): # collects the unique ids with their respective data streams\n",
    "\n",
    "    num_unique_ids = len(indices)\n",
    "    all_ids = []\n",
    "    for x in range(num_unique_ids):\n",
    "         \n",
    "         id_data = data[indices[x]]\n",
    "         all_ids.append(id_data)\n",
    "\n",
    "    return all_ids\n",
    "\n",
    "all_id_data = get_data_stream(indices,data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # combines each unique id with its data stream in a matrix where the ID is in the first row, and its data stream is (ordered)\n",
    " # in the column under it. (one column = ID: data1,data2,..). column length = max data length, if data doesn't fill upp all rows they are filled with nan. \n",
    "def combine_ids_data(indices,id,all_id_data):\n",
    "    i = -1\n",
    "    ids_and_data = np.ones((np.max([len(pi) for pi in all_id_data])+1,len(all_id_data)))*np.nan \n",
    "\n",
    "    for x in indices:\n",
    "        i = i+ 1\n",
    "        index = indices[i] \n",
    "        index = index[0] # only save first one in group\n",
    "        ID = id[index]\n",
    "\n",
    "        data_stream = all_id_data[i]\n",
    "        data_stream = data_stream[:,0] \n",
    "        ids_and_data[0,i] = ID\n",
    "        ids_and_data[1:len(data_stream)+1,i] = data_stream\n",
    "\n",
    "    return ids_and_data\n",
    "\n",
    "d = combine_ids_data(indices,id,all_id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Number of individual IDs in the full sequence = {len(d[0,:])}')\n",
    "print(f'Maximum number of data packets per ID = {np.max([len(pi) for pi in all_id_data])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(100),d[0:100,0],'-o')"
   ]
  },
  {
   "source": [
    "# Copied"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "from numpy import array\n",
    "\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(input): # remove nans from data stream column\n",
    "    input_nans = np.isnan(input) \n",
    "    not_nan = ~ input_nans\n",
    "    clean_input = input[not_nan]\n",
    "    return clean_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        end_iy = end_ix + n_steps \n",
    "\t\t# check if we are beyond the sequence\n",
    "        if end_iy > len(sequence)-1:\n",
    "            break\n",
    "\t\t# gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:end_iy]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "seq = remove_nans(d[:,0])   \n",
    "\n",
    "nTrain = round(0.9*len(seq))\n",
    "nTest = len(seq)-nTrain\n",
    "xTrain = seq[0:nTrain]\n",
    "xTest = seq[nTrain:len(seq)]\n",
    "\n",
    "X_train, y_train = split_sequence(xTrain, n_steps)\n",
    "X_test, y_test = split_sequence(xTest, n_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow\n",
    "#! pip install keras\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define model (autoencoder)\n",
    "n_steps = 5\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_steps,1)))\n",
    "print(model.output_shape)\n",
    "model.add(RepeatVector(n_steps))\n",
    "print(model.output_shape)\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "source": [
    "# CANnolo\n",
    "Will probably need to use the functional API and not the sequential model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# canolo\n",
    "model = Sequential()\n",
    "\n",
    "# Encoder\n",
    "model.add(Dense(256, activation='tanh', input_shape=(n_steps,1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "#model.add(RepeatVector(n_steps))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "#model.add(RepeatVector(n_steps))\n",
    "\n",
    "# Decoder\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "#model.add(RepeatVector(n_steps))\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(256, activation= 'sigmoid')))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "#model.add(Dense(1)) # they don't mention this in the article but dimensions are wrong otherwise\n",
    "\n",
    "#model.compile(optimizer='adam', loss='BinaryCrossentropy')\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[0.01145735]\n  [0.03497278]\n  [0.07128196]]\n\n [[0.05030405]\n  [0.11452433]\n  [0.1897668 ]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from numpy import array\n",
    "# define model\n",
    "inputs1 = Input(shape=(3, 1))\n",
    "lstm1 = LSTM(1,return_sequences=True)(inputs1)\n",
    "model = Model(inputs=inputs1, outputs=lstm1)\n",
    "# define input data\n",
    "data = array([0.1, 0.2, 0.3,0.4,0.5,0.6]).reshape((2,3,1))\n",
    "# make and show prediction\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm_68 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"Autoencoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 3, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 3, 256)       512         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 3, 256)       0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_66 (LSTM)                  (None, 3, 128)       197120      dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_67 (LSTM)                  [(None, 3, 128), (No 131584      lstm_66[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_25 (TensorFl [(None, 512)]        0           lstm_67[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_50 (TensorFlo [(None, 512)]        0           tf_op_layer_MatMul_25[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BiasAdd_25 (TensorF [(None, 512)]        0           tf_op_layer_AddV2_50[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_25 (TensorFlo [(None, 128), (None, 0           tf_op_layer_BiasAdd_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_76 (TensorF [(None, 128)]        0           tf_op_layer_split_25[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_75 (TensorF [(None, 128)]        0           tf_op_layer_split_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_50 (TensorFlow [(None, 128)]        0           tf_op_layer_split_25[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_75 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_76[0][0]     \n",
      "                                                                 lstm_67[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_76 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_75[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_51 (TensorFlo [(None, 128)]        0           tf_op_layer_Mul_75[0][0]         \n",
      "                                                                 tf_op_layer_Mul_76[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_77 (TensorF [(None, 128)]        0           tf_op_layer_split_25[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_51 (TensorFlow [(None, 128)]        0           tf_op_layer_AddV2_51[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_77 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_77[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_51[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_26 (TensorFl [(None, 512)]        0           tf_op_layer_Mul_77[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_52 (TensorFlo [(None, 512)]        0           tf_op_layer_MatMul_26[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BiasAdd_26 (TensorF [(None, 512)]        0           tf_op_layer_AddV2_52[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_26 (TensorFlo [(None, 128), (None, 0           tf_op_layer_BiasAdd_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_79 (TensorF [(None, 128)]        0           tf_op_layer_split_26[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_78 (TensorF [(None, 128)]        0           tf_op_layer_split_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_52 (TensorFlow [(None, 128)]        0           tf_op_layer_split_26[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_78 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_79[0][0]     \n",
      "                                                                 tf_op_layer_AddV2_51[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_79 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_78[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_53 (TensorFlo [(None, 128)]        0           tf_op_layer_Mul_78[0][0]         \n",
      "                                                                 tf_op_layer_Mul_79[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_80 (TensorF [(None, 128)]        0           tf_op_layer_split_26[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_53 (TensorFlow [(None, 128)]        0           tf_op_layer_AddV2_53[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_80 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_80[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_53[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_27 (TensorFl [(None, 512)]        0           tf_op_layer_Mul_80[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_54 (TensorFlo [(None, 512)]        0           tf_op_layer_MatMul_27[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BiasAdd_27 (TensorF [(None, 512)]        0           tf_op_layer_AddV2_54[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_27 (TensorFlo [(None, 128), (None, 0           tf_op_layer_BiasAdd_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_82 (TensorF [(None, 128)]        0           tf_op_layer_split_27[0][1]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_81 (TensorF [(None, 128)]        0           tf_op_layer_split_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_54 (TensorFlow [(None, 128)]        0           tf_op_layer_split_27[0][2]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_81 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_82[0][0]     \n",
      "                                                                 tf_op_layer_AddV2_53[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_82 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_81[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_55 (TensorFlo [(None, 128)]        0           tf_op_layer_Mul_81[0][0]         \n",
      "                                                                 tf_op_layer_Mul_82[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_83 (TensorF [(None, 128)]        0           tf_op_layer_split_27[0][3]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tanh_55 (TensorFlow [(None, 128)]        0           tf_op_layer_AddV2_55[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_83 (TensorFlowO [(None, 128)]        0           tf_op_layer_Sigmoid_83[0][0]     \n",
      "                                                                 tf_op_layer_Tanh_55[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_packed_6 (TensorFlo [(3, 1, 128)]        0           tf_op_layer_Mul_77[0][0]         \n",
      "                                                                 tf_op_layer_Mul_80[0][0]         \n",
      "                                                                 tf_op_layer_Mul_83[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Transpose_6 (Tensor [(1, 3, 128)]        0           tf_op_layer_packed_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_69 (LSTM)                  (1, 3, 128)          131584      tf_op_layer_Transpose_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (1, 3, 256)          33024       lstm_69[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_70 (LSTM)                  (1, 3, 128)          197120      dense_30[0][0]                   \n",
      "                                                                 lstm_67[0][1]                    \n",
      "                                                                 lstm_67[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                  (1, 3, 128)          131584      lstm_70[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (1, 3, 256)          33024       lstm_71[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 855,552\n",
      "Trainable params: 855,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "# define input data\n",
    "#data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
    "input_data = array([0.1, 0.2, 0.3,0.4,0.5,0.6]).reshape((2,3,1))\n",
    "window_length = 3\n",
    "num_samples = 2\n",
    "\n",
    "# define Encoder\n",
    "\n",
    "inputs1 = Input(shape=(3, 1))\n",
    "dense1 = Dense(256, activation='tanh')(inputs1)\n",
    "dropout = Dropout(0.2)(dense1)\n",
    "lstm1 = LSTM(128,return_sequences=True)(dropout)\n",
    "lstm2, state_h, state_c = LSTM(128,return_sequences=True,return_state=True)(lstm1)\n",
    "states = [state_h, state_c]\n",
    "\n",
    "# define Decoder\n",
    "decoder_input = np.zeros((1,3,1))\n",
    "for _ in range(num_samples):\n",
    "    # Want to run the decoder on one time window at a time\n",
    "    lstm3 =  LSTM(128,return_sequences=True)(decoder_input, initial_state=states)\n",
    "    lstm4 = LSTM(128,return_sequences=True)(lstm3)\n",
    "    dense2 = Dense(256, activation='sigmoid')(lstm4)\n",
    "    decoder_input = dense2\n",
    "    states = [state_h, state_c]\n",
    "\n",
    "output = dense2\n",
    "#lstm3 = LSTM(128,return_sequences=True)([lstm2, state_h, state_c])\n",
    "#lstm4, state_h, state_c = LSTM(128,return_sequences=True,return_state=True)(lstm3)\n",
    "#dense2 = Dense(256, activation='sigmoid')(lstm4)\n",
    "\n",
    "EncoderDecoder = Model(inputs=inputs1, outputs=output,name=\"Autoencoder\")\n",
    "\n",
    "\n",
    "EncoderDecoder.summary()\n",
    "\n"
   ]
  },
  {
   "source": [
    "from matplotlib import pyplot\n",
    "# design network\n",
    "# fit network\n",
    "# batch size should be 128\n",
    "model = EncoderDecoder\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=1000)\n",
    "for x in range(num_samples):\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=15, validation_data=(X_test, y_test), verbose=2, shuffle=False,callbacks=[es])\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d8a32d371026>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# plot history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "n_steps = 5\n",
    "nums = array([1,2,3,4,5,6,7,8,9,10,11,12,13,141,5])\n",
    "test_nums = array([1, 1, 1, 2, 6, 2, 4, 2, 3, 8, 7, 6, 2, 3, 14])\n",
    "rev_nums = nums[::-1]\n",
    "rev_test_nums = test_nums[::-1]\n",
    "n_features = 1\n",
    "#X_test = test_nums.reshape((test_nums.shape[0], test_nums.shape[1], n_features))\n",
    "#y_test = rev_test_nums.reshape((rev_test_nums.shape[0], rev_test_nums.shape[1], n_features))\n",
    "#X_train = nums.reshape((nums.shape[0], nums.shape[1], n_features))\n",
    "#y_train = rev_nums.reshape((rev_nums.shape[0], rev_nums.shape[1], n_features))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# nums = nums.reshape(-1, 1)\n",
    "# rev_test_nums = rev_test_nums.reshape(-1,1)\n",
    "# test_nums = test_nums.reshape(-1,1)\n",
    "# rev_nums = rev_nums.reshape(-1,1)\n",
    "\n",
    "# nums = scaler.fit_transform(nums)\n",
    "# rev_nums = scaler.fit_transform(rev_nums)\n",
    "\n",
    "# test_nums = scaler.fit_transform(test_nums)\n",
    "# rev_test_nums = scaler.fit_transform(rev_test_nums)\n",
    "\n",
    "X_test = test_nums.reshape((3, n_steps, n_features))\n",
    "y_test = rev_test_nums.reshape((3, n_steps, n_features))\n",
    "X_train = nums.reshape((3, n_steps, n_features))\n",
    "y_train = rev_nums.reshape((3, n_steps, n_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# demonstrate prediction\n",
    "x_input = array([1,2,3,4,5,6,7,8,9,10,11,12,13,141,5])\n",
    "x_input = x_input.reshape(3,5,1)\n",
    "#print(x_input)\n",
    "#x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,))\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "a = dense(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(a)\n",
    "outputs = layers.Dense(10)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "model.summary()\n",
    "\n"
   ]
  }
 ]
}